{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e318b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from configs import stronger_tradeoff\n",
    "from reward_model import CondensedRewardModel\n",
    "from algorithms import TransitionModel, DumbTransitionModel, update_transition_matrix\n",
    "import time\n",
    "import itertools\n",
    "from algorithms import update_reward_model\n",
    "from transition_model import OracleTransitionModel\n",
    "from gyms.sim_library import get_stronger_tradeoff_simulator\n",
    "import torch\n",
    "import numpy.random as npr\n",
    "from gyms.sim_library import stronger_reward_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "cfg = stronger_tradeoff\n",
    "state_p0 = cfg['state_p0']\n",
    "\n",
    "action_names = cfg['action_names']\n",
    "state_names = cfg['state_names']\n",
    "delta_names = range(1, 11)\n",
    "\n",
    "actions = list(range(len(action_names)))\n",
    "states = list(range(len(state_names)))\n",
    "deltas = list(range(len(delta_names)))\n",
    "\n",
    "\n",
    "gamma = 0.99\n",
    "action_cost = 5\n",
    "horizon = 50\n",
    "\n",
    "env = get_stronger_tradeoff_simulator(\n",
    "    actions, states, state_p0, \n",
    "    gamma=gamma, \n",
    "    action_cost=action_cost, \n",
    "    horizon=horizon, \n",
    "    terminal_reward=0)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "dumb = False\n",
    "terminal_state = env.terminal_state\n",
    "is_windygrid = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31230de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_expanded_T(T, num_deltas):\n",
    "    A, S1, S2 = T.shape\n",
    "    assert S1 == S2\n",
    "    expanded_T = torch.ones(A, S1, S2, num_deltas)\n",
    "    for a in range(A):\n",
    "        for k in range(num_deltas):\n",
    "            T_delta = torch.matrix_power(T[a, :, :], k + 1)\n",
    "            expanded_T[a, :, :, k] = T_delta\n",
    "    return expanded_T\n",
    "\n",
    "\n",
    "def sample_reward(T, s, a, delay, terminal_state, terminal_reward, cost, gamma, horizon, reward_function):\n",
    "    T_a = T[a]\n",
    "    reward = 0\n",
    "    cur_s = s\n",
    "    cur_t = 0  # steps taken\n",
    "    terminated = (cur_s == terminal_state)\n",
    "\n",
    "    if terminated:\n",
    "        reward += terminal_reward\n",
    "        return cur_s, reward, cur_t, terminated\n",
    "\n",
    "    reward -= cost\n",
    "    for _ in range(delay):\n",
    "        discount = gamma ** cur_t\n",
    "        reward += discount * reward_function(cur_s, a)\n",
    "        cur_s = npr.multinomial(1, T_a[cur_s], size=None).tolist().index(1)\n",
    "        cur_t += 1\n",
    "        if cur_s == terminal_state:\n",
    "            terminated = True\n",
    "            break\n",
    "            \n",
    "    return cur_s, reward, cur_t, terminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eabd4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e0b210",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "reps = 30\n",
    "Ns = [1, 2, 5, 10, 20, 50, 100]\n",
    "terminal_state = env.terminal_state\n",
    "terminal_reward = env.terminal_reward\n",
    "\n",
    "def R_to_R_lookup(R, transition):\n",
    "    R_lookup = {}\n",
    "    for (s, a, k) in itertools.product(states, actions, deltas):\n",
    "        expected_R = R.get_prediction(s, a, k, transition)                \n",
    "        R_lookup[(s, a, k)] = expected_R\n",
    "    return R_lookup\n",
    "\n",
    "\n",
    "# get all oracle expected rewards\n",
    "oracle_R = CondensedRewardModel(\n",
    "    states, actions, deltas, delta_names,\n",
    "    terminal_state, gamma, action_cost, device,\n",
    "    is_windygrid=is_windygrid, env=env, \n",
    "    oracle_reward_lookup=True).to(device)\n",
    "\n",
    "oracle_transition = OracleTransitionModel(env, actions, states, delta_names, device, dumb=False)\n",
    "oracle_T = oracle_transition()\n",
    "expanded_true_T = get_expanded_T(oracle_T, len(delta_names))\n",
    "oracle_R_lookup = R_to_R_lookup(oracle_R, oracle_transition)\n",
    "\n",
    "reward_function = lambda s, a: stronger_reward_function(s, a, terminal_reward=terminal_reward)\n",
    "\n",
    "R_lr = 0.2\n",
    "T_lr = 0.01\n",
    "convergence = 1e-5\n",
    "patience = 3\n",
    "model_batchsize = 500\n",
    "batchsize = model_batchsize\n",
    "weight_by_state_action = False\n",
    "max_iters = 5000\n",
    "weight_by_actions = False\n",
    "\n",
    "print(terminal_state, terminal_reward, states, actions, deltas)\n",
    "all_R_oracleT_lookups = {}\n",
    "all_R_smartT_lookups = {}\n",
    "all_R_dumbT_lookups = {}\n",
    "all_empirical = {}\n",
    "for i, N in enumerate(Ns):\n",
    "    print('N: ', N, 'deltas: ', deltas)\n",
    "    R_oracleT_lookups = []\n",
    "    R_smartT_lookups = []\n",
    "    R_dumbT_lookups = []\n",
    "    R_empiric = []\n",
    "    \n",
    "    smart_Ts = []\n",
    "    dumb_Ts = []\n",
    "    empiric_Ts = []\n",
    "    for rep in range(reps):\n",
    "        print(f'----- N: {N}, rep {rep} ({time.time() - start:.2f} sec) ------')\n",
    "        dataset = []\n",
    "        empirical_rs = {}\n",
    "        for (s, a, k) in itertools.product(states, actions, deltas):\n",
    "            for _ in range(N):\n",
    "                delay = k + 1\n",
    "                s2, r, cur_t, terminated = sample_reward(oracle_T, s, a, delay, \n",
    "                                                         terminal_state, terminal_reward, \n",
    "                                                         action_cost, gamma, horizon, reward_function)\n",
    "                dataset.append((a, k, s, s2, r))\n",
    "                empirical_rs[(s, a, k)] = empirical_rs.get((s, a, k), []) + [r]\n",
    "        dataset = shuffle(dataset)\n",
    "        \n",
    "        oracle_transition = OracleTransitionModel(env, actions, states, delta_names, device, dumb=False)\n",
    "        oracle_T = oracle_transition()\n",
    "        \n",
    "        ## condensed reward + oracle transition\n",
    "        R_oracleT = CondensedRewardModel(\n",
    "            states, actions, deltas, delta_names,\n",
    "            terminal_state, gamma, action_cost, device,\n",
    "            is_windygrid=is_windygrid, env=env, \n",
    "            oracle_reward_lookup=False).to(device)\n",
    "        r_loss_func = torch.nn.MSELoss()\n",
    "        r_optimizer = torch.optim.Adam(R_oracleT.parameters(), lr=R_lr)\n",
    "\n",
    "        R_oracleT, avg_r_loss = update_reward_model(\n",
    "            dataset, R_oracleT, r_loss_func, r_optimizer, device,\n",
    "            convergence=convergence, \n",
    "            patience=patience, \n",
    "            batchsize=model_batchsize,\n",
    "            max_iters=max_iters,\n",
    "            transition_model=oracle_transition,\n",
    "            weight_by_actions=weight_by_actions)\n",
    "        R_oracleT_lookup = R_to_R_lookup(R_oracleT, oracle_transition)\n",
    "        R_oracleT_lookups.append(R_oracleT_lookup)\n",
    "        \n",
    "        ## condensed reward + smart transition\n",
    "        smart_transition = TransitionModel(actions, states, delta_names, device, terminal_state)\n",
    "        smart_optimizer = torch.optim.SGD(smart_transition.parameters(), lr=T_lr)\n",
    "        smart_transition, nll_avg = update_transition_matrix(\n",
    "            dataset, \n",
    "            smart_transition, \n",
    "            smart_optimizer,\n",
    "            torch.device('cpu'),\n",
    "            convergence=convergence, \n",
    "            patience=patience, \n",
    "            batchsize=batchsize,\n",
    "            weight_by_actions=weight_by_actions,\n",
    "            weight_by_state_action=weight_by_state_action) \n",
    "        est_T = smart_transition()\n",
    "        expanded_est_T = get_expanded_T(est_T, len(delta_names))\n",
    "        smart_Ts.append(expanded_est_T)\n",
    "  \n",
    "        R_smartT = CondensedRewardModel(\n",
    "            states, actions, deltas, delta_names,\n",
    "            terminal_state, gamma, action_cost, device,\n",
    "            is_windygrid=is_windygrid, env=env, \n",
    "            oracle_reward_lookup=False).to(device)\n",
    "        r_smartT_optimizer = torch.optim.Adam(R_smartT.parameters(), lr=R_lr)\n",
    "\n",
    "        R_smartT, avg_r_loss = update_reward_model(\n",
    "            dataset, R_smartT, r_loss_func, r_smartT_optimizer, device,\n",
    "            convergence=convergence, \n",
    "            patience=patience, \n",
    "            batchsize=model_batchsize,\n",
    "            max_iters=max_iters,\n",
    "            transition_model=smart_transition,\n",
    "            weight_by_actions=weight_by_actions)\n",
    "        R_smartT_lookup = R_to_R_lookup(R_smartT, smart_transition)\n",
    "        R_smartT_lookups.append(R_smartT_lookup)\n",
    "        \n",
    "        ## condensed reward + dumb transition\n",
    "        dumb_transition = DumbTransitionModel(actions, states, delta_names, device, terminal_state)\n",
    "        dumb_optimizer = torch.optim.SGD(dumb_transition.parameters(), lr=T_lr)\n",
    "        dumb_transition, nll_avg = update_transition_matrix(\n",
    "            dataset, \n",
    "            dumb_transition, \n",
    "            dumb_optimizer,\n",
    "            torch.device('cpu'),\n",
    "            convergence=convergence, \n",
    "            patience=patience, \n",
    "            batchsize=batchsize,\n",
    "            weight_by_actions=weight_by_actions,\n",
    "            weight_by_state_action=weight_by_state_action) \n",
    "        expanded_est_T = dumb_transition()\n",
    "        expanded_est_T = expanded_est_T.permute(0, 2, 3, 1)\n",
    "        dumb_Ts.append(expanded_est_T)\n",
    "        \n",
    "        R_dumbT = CondensedRewardModel(\n",
    "            states, actions, deltas, delta_names,\n",
    "            terminal_state, gamma, action_cost, device,\n",
    "            is_windygrid=is_windygrid, env=env, \n",
    "            oracle_reward_lookup=False).to(device)\n",
    "        r_dumbT_optimizer = torch.optim.Adam(R_dumbT.parameters(), lr=R_lr)\n",
    "\n",
    "        R_dumbT, avg_r_loss = update_reward_model(\n",
    "            dataset, R_dumbT, r_loss_func, r_dumbT_optimizer, device,\n",
    "            convergence=convergence, \n",
    "            patience=patience, \n",
    "            batchsize=model_batchsize,\n",
    "            max_iters=max_iters,\n",
    "            transition_model=dumb_transition,\n",
    "            weight_by_actions=weight_by_actions)\n",
    "        R_dumbT_lookup = R_to_R_lookup(R_dumbT, dumb_transition)\n",
    "        R_dumbT_lookups.append(R_dumbT_lookup)\n",
    "        \n",
    "        empirical_rs = {tup: np.mean(rs) for (tup, rs) in empirical_rs.items()}\n",
    "        R_empiric.append(empirical_rs)\n",
    "        \n",
    "        print('R_oracleT estimated lookup: ', R_oracleT.reward_lookup)\n",
    "        print('R_smartT estimated lookup: ', R_smartT.reward_lookup)\n",
    "        print('R_dumbT estimated lookup: ', R_dumbT.reward_lookup)\n",
    "        print('true lookup: ', oracle_R.reward_lookup)\n",
    "            \n",
    "    all_R_oracleT_lookups[N] = R_oracleT_lookups\n",
    "    all_R_smartT_lookups[N] = R_smartT_lookups\n",
    "    all_R_dumbT_lookups[N] = R_dumbT_lookups\n",
    "    all_empirical[N] = R_empiric\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06c7f65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "def plot_comparison(ax, all_R_lookups, label=''):\n",
    "    # compute comparison to oracle expected aggregate rewards\n",
    "    N_to_dists = {}\n",
    "    for N, R_lookups in all_R_lookups.items():\n",
    "        dists = []\n",
    "        for R_lookup in R_lookups:\n",
    "            dist = []\n",
    "            for (s, a, k), g in R_lookup.items():\n",
    "                gstar = oracle_R_lookup[(s, a, k)]\n",
    "                dist.append(abs(g - gstar))\n",
    "            dists.append(max(dist).item())\n",
    "        N_to_dists[N] = dists\n",
    "\n",
    "    means = np.array([np.mean(N_to_dists[N]) for N in Ns])\n",
    "    stds = np.array([np.std(N_to_dists[N]) / np.sqrt(reps) for N in Ns])\n",
    "\n",
    "    ax.plot(Ns, means, 'o-', label=label)\n",
    "    ax.fill_between(Ns, means - 1.69 * stds, means + 1.69 * stds, alpha=0.1)\n",
    "    \n",
    "plot_comparison(ax, all_R_oracleT_lookups, label=r'learning $\\widehat{R}$, oracle $\\widehat{P}$')\n",
    "plot_comparison(ax, all_R_smartT_lookups, label=r'learning $\\widehat{R}$, timing-aware $\\widehat{P}$')\n",
    "plot_comparison(ax, all_R_dumbT_lookups, label=r'learning $\\widehat{R}$, timing-naive $\\widehat{P}$')\n",
    "plot_comparison(ax, all_empirical, label=r'empirical avg')\n",
    "\n",
    "\n",
    "ax.legend(fontsize=14)\n",
    "ax.set_title(r'$||\\mathcal{G}_{P, R} - \\mathcal{G}_{\\widehat{P}, \\widehat{R}}||_{\\infty}$ vs. n', fontsize=16)\n",
    "ax.set_xlabel('n', fontsize=14)\n",
    "ax.set_ylabel('Estimation error', fontsize=14)\n",
    "ax.set_yscale('log')\n",
    "plt.savefig('reward_estimation_error.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11625e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4d2c98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f2f2b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b919435",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
